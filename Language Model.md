#### [StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://arxiv.org/abs/1908.04577)
- they force bert model to capture structure information by training bert on two  auxiliary tasks including predicting the text sequence when the order of some input words are shuffled or the order of some input sentences are shuffled.

#### [Learning Structured Text Representations](https://www.aclweb.org/anthology/Q18-1005.pdf)
- this paper tries to learn the representation of documents while incorporating the document structures into representation. the document structures are represented by Rhetorical Structure Theory, which depicts the dependency relations between setences, and it is non-projective dependency tree. Therefore, they propose a model to handle the RST tree of documents.

