#### [StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://arxiv.org/abs/1908.04577)
- they force bert model to capture structure information by training bert on two  auxiliary tasks including predicting the text sequence when the order of some input words 
are shuffled or the order of some input sentences are shuffled.
